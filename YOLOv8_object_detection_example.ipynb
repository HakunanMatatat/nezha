{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 Object Detection Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates a complete workflow for object detection using YOLOv8, a state-of-the-art, real-time object detection model. We will cover the essential steps from setting up the environment to performing inference with a fine-tuned model.\n",
    "\n",
    "**Dataset:** We will use the **COCO8 dataset**, a small subset of the COCO (Common Objects in Context) dataset, provided by Ultralytics. It contains 8 images (4 for training, 4 for validation) and includes the standard 80 COCO object classes. This dataset is ideal for quick tests and demonstrations.\n",
    "\n",
    "**Steps Covered:**\n",
    "1.  **Setup:** Installing necessary libraries (`ultralytics` for YOLOv8, `PyTorch`, `OpenCV`).\n",
    "2.  **Data Preparation:** Downloading the dataset, understanding its structure, and preparing the `data.yaml` configuration file required for YOLOv8 training.\n",
    "3.  **Model Training (Fine-tuning):** Loading a pre-trained YOLOv8 model and fine-tuning it on our custom dataset.\n",
    "4.  **Inference and Visualization:** Using the fine-tuned model to detect objects in new images and visualizing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ultralytics for YOLOv8\n",
    "# This package contains the YOLOv8 model implementation and utilities.\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install PyTorch and torchvision\n",
    "# YOLOv8 is built on PyTorch. Torchvision provides useful image transformation tools.\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Install OpenCV for image processing\n",
    "# OpenCV is used for image loading, manipulation, and displaying results.\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above installs the essential packages: `ultralytics` for YOLOv8, `torch` and `torchvision` as its deep learning framework, and `opencv-python-headless` for image processing tasks. Using `opencv-python-headless` is often preferred in server or container environments where GUI features are not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we use the **COCO8 dataset**, a small subset of the COCO (Common Objects in Context) dataset provided by Ultralytics. It's designed for quick testing and includes 8 images (4 for training, 4 for validation) across the 80 standard COCO classes. This allows for a rapid demonstration of the training and inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the COCO8 dataset\n",
    "# The '-q' flag for wget makes it operate quietly.\n",
    "# '-O coco8.zip' specifies the output filename.\n",
    "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/coco8.zip -O coco8.zip\n",
    "\n",
    "# Unzip the dataset\n",
    "# The '-q' flag makes unzip operate quietly.\n",
    "# The '-d ./datasets/' extracts files into the './datasets/' directory, creating 'coco8' inside it.\n",
    "!unzip -q coco8.zip -d ./datasets/\n",
    "\n",
    "# Remove the zip file after extraction to save space\n",
    "!rm coco8.zip \n",
    "\n",
    "# List the contents of the dataset directory to understand its structure\n",
    "!ls ./datasets/coco8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COCO8 dataset has been downloaded and unzipped into the `./datasets/coco8` directory. \n",
    "The structure of COCO8 typically is:\n",
    "- `images/train/`: Directory with training images.\n",
    "- `images/val/`: Directory with validation images.\n",
    "- `labels/train/`: Directory with corresponding label files for training images.\n",
    "- `labels/val/`: Directory with corresponding label files for validation images.\n",
    "- Each image file (e.g., `image.jpg`) has a corresponding text file (`image.txt`) in the respective `labels` subdirectory, where each line in the text file defines a bounding box (`class_id x_center y_center width height`).\n",
    "- A `coco8.yaml` file is often included, which we will essentially replicate as `custom_data.yaml` but with absolute paths for robustness in Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of `data.yaml`\n",
    "\n",
    "The `data.yaml` file is crucial for YOLOv8 as it provides the model with essential information about the dataset:\n",
    "\n",
    "- `path`: An absolute path to the root directory of the dataset (e.g., `/content/datasets/coco8`).\n",
    "- `train`: Path to the directory containing training images, relative to `path` (e.g., `images/train`).\n",
    "- `val`: Path to the directory containing validation images, relative to `path` (e.g., `images/val`).\n",
    "- `test` (optional): Path to the directory containing test images, relative to `path`.\n",
    "- `names`: A dictionary or list of class names, where the index corresponds to the class ID. COCO8 uses the 80 standard COCO classes.\n",
    "\n",
    "We will create our own `custom_data.yaml` file in the Colab root directory (`/content/`) to explicitly define these parameters, ensuring clarity and correct pathing for the YOLOv8 training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Define the absolute path to the COCO8 dataset in Colab environment\n",
    "dataset_path = '/content/datasets/coco8'\n",
    "\n",
    "# Define the COCO class names (80 classes)\n",
    "coco_names = {\n",
    "    0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat',\n",
    "    9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n",
    "    16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack',\n",
    "    25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
    "    33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket',\n",
    "    39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple',\n",
    "    48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake',\n",
    "    56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop',\n",
    "    64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink',\n",
    "    72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier',\n",
    "    79: 'toothbrush'\n",
    "}\n",
    "\n",
    "# Create the dictionary for our custom_data.yaml content\n",
    "custom_data_yaml_content = {\n",
    "    'path': dataset_path,\n",
    "    'train': 'images/train',  # Relative to 'path'\n",
    "    'val': 'images/val',      # Relative to 'path'\n",
    "    # COCO8 typically doesn't have a separate 'test' set in its default zip,\n",
    "    # so we'll omit it or one could point 'test' to 'images/val' if needed.\n",
    "    'names': coco_names\n",
    "}\n",
    "\n",
    "# Define the name for our custom YAML file (to be created in /content/)\n",
    "output_yaml_path = 'custom_data.yaml'\n",
    "\n",
    "# Write the custom_data.yaml file\n",
    "with open(output_yaml_path, 'w') as f:\n",
    "    yaml.dump(custom_data_yaml_content, f, sort_keys=False, default_flow_style=None)\n",
    "\n",
    "print(f\"--- Content of created {output_yaml_path} ---\")\n",
    "with open(output_yaml_path, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Verify that the dataset path and image directories exist\n",
    "print(f\"\\nChecking dataset paths:\")\n",
    "print(f\"Dataset root exists: {os.path.exists(dataset_path)}\")\n",
    "print(f\"Train images directory exists: {os.path.exists(os.path.join(dataset_path, custom_data_yaml_content['train']))}\")\n",
    "print(f\"Validation images directory exists: {os.path.exists(os.path.join(dataset_path, custom_data_yaml_content['val']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above performs the following actions:\n",
    "1.  Defines the absolute path to where the COCO8 dataset is stored (`/content/datasets/coco8`).\n",
    "2.  Defines a dictionary `coco_names` containing all 80 standard COCO class names.\n",
    "3.  Constructs the content for `custom_data.yaml`, specifying the dataset `path`, relative paths for `train` and `val` image directories, and the `names` dictionary.\n",
    "4.  Writes this configuration to `custom_data.yaml` in the Colab root directory (`/content/custom_data.yaml`). This file will be used by YOLOv8 for training.\n",
    "5.  Prints the content of the created YAML file and verifies the existence of the specified dataset paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training (Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os # os is already imported, but good to have it here if this cell is run independently\n",
    "\n",
    "# Load a pre-trained YOLOv8 model.\n",
    "# 'yolov8n.pt' is the nano version, which is small and fast, suitable for quick demonstrations.\n",
    "# Other versions like 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt' offer higher accuracy at the cost of size and speed.\n",
    "# Using a '.pt' file loads pre-trained weights. Using a '.yaml' file (e.g., 'yolov8n.yaml') would build the model from scratch based on its architecture definition.\n",
    "model = YOLO('yolov8n.pt') \n",
    "\n",
    "# Display model information (optional, but useful for verification)\n",
    "# This shows the model architecture, number of layers, parameters, and GFLOPs.\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading `yolov8n.pt`, which is a YOLOv8 nano model pre-trained on the COCO dataset. By using a pre-trained model, we leverage **transfer learning**.\n",
    "\n",
    "**Benefits of Transfer Learning:**\n",
    "- **Faster Training:** The model has already learned general visual features (edges, textures, basic shapes) from a large dataset. This allows it to learn the specifics of our new, smaller dataset much faster.\n",
    "- **Less Data Required:** Fine-tuning often achieves good results with less data than training a model from scratch, as the foundational feature extraction capabilities are already present.\n",
    "- **Potentially Better Performance:** Pre-trained models provide a strong starting point, which can lead to better overall performance, especially when the custom dataset is small or similar in nature to the pre-training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the custom_data.yaml file created in the previous step.\n",
    "# This file is located in the root directory of our notebook environment.\n",
    "data_yaml_path = 'custom_data.yaml' \n",
    "\n",
    "# Train (fine-tune) the model on our custom dataset.\n",
    "results = model.train(\n",
    "    data=data_yaml_path,  # Path to our custom_data.yaml file.\n",
    "    epochs=25,            # Number of training epochs. Keep low for a quick demo (e.g., 25-50). For production, this would be higher (e.g., 100-300).\n",
    "    imgsz=640,            # Input image size. Images will be resized to 640x640 pixels.\n",
    "    batch=8,              # Batch size. Adjust based on available GPU memory. Common values are 8, 16, 32.\n",
    "    name='yolov8n_coco8_finetuned', # Name for the training run. Results will be saved in 'runs/detect/yolov8n_coco8_finetuned'.\n",
    "    patience=5            # Early stopping patience. Training will stop if no improvement in validation metrics is observed for 5 consecutive epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.train()` function initiates the fine-tuning process. \n",
    "\n",
    "**Key Training Parameters Explained:**\n",
    "- `data`: Specifies the path to our `custom_data.yaml` file, which defines the dataset structure and class information.\n",
    "- `epochs`: The number of complete passes through the entire training dataset. For this demonstration, we use 25 epochs. In real-world scenarios, this value might be significantly higher (e.g., 100, 200, or more), depending on the dataset's size and complexity, and monitored by observing validation metrics.\n",
    "- `imgsz`: The input image size (height and width) for the model. All training images are resized to this dimension (e.g., 640x640 pixels).\n",
    "- `batch`: The number of images processed together in one forward/backward pass during training. This should be set based on available GPU memory; larger batch sizes can sometimes lead to more stable gradients but require more memory.\n",
    "- `name`: A custom name for this specific training run. YOLOv8 will create a directory with this name (e.g., `runs/detect/yolov8n_coco8_finetuned`) to store all training artifacts, including model weights, logs, and visualizations.\n",
    "- `patience`: Enables early stopping. If the validation metrics do not show improvement for a specified number of epochs (here, 5), the training process will halt. This helps prevent overfitting and can save computational resources if the model has converged.\n",
    "\n",
    "All training results, including the best model weights (`best.pt`), final model weights (`last.pt`), performance metrics (like mAP), confusion matrices, and example validation images with predictions, will be saved in a directory structured as `runs/detect/<name_argument_value>` (e.g., `runs/detect/yolov8n_coco8_finetuned`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for inference and display\n",
    "from ultralytics import YOLO # YOLO class for model loading and prediction\n",
    "import cv2                  # OpenCV for image reading and color conversion\n",
    "import matplotlib.pyplot as plt # Matplotlib for displaying images in the notebook\n",
    "import os                   # For path manipulation and directory listing\n",
    "\n",
    "# Path to the best trained model weights.\n",
    "# This path is determined by the 'name' argument used during 'model.train()'.\n",
    "# The 'best.pt' file contains the model weights that achieved the best validation performance.\n",
    "fine_tuned_model_path = 'runs/detect/yolov8n_coco8_finetuned/weights/best.pt'\n",
    "\n",
    "# Initialize model variable\n",
    "inference_model = None \n",
    "\n",
    "# Check if the trained model file exists\n",
    "if not os.path.exists(fine_tuned_model_path):\n",
    "    print(f\"Error: Trained model weights not found at {fine_tuned_model_path}\")\n",
    "    print(\"Please ensure that:\")\n",
    "    print(\"1. The training process in the previous section completed successfully.\")\n",
    "    print(\"2. The 'name' argument in model.train() ('yolov8n_coco8_finetuned') matches the path used here.\")\n",
    "    print(\"3. The 'best.pt' file was generated in the specified directory.\")\n",
    "    # If the model isn't found, inference_model remains None, and subsequent cells should handle this.\n",
    "else:\n",
    "    # Load the fine-tuned model for inference\n",
    "    inference_model = YOLO(fine_tuned_model_path)\n",
    "    print(f\"Successfully loaded fine-tuned model from {fine_tuned_model_path}\")\n",
    "    # Optionally, display model information to confirm it's the correct one\n",
    "    # inference_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model weights that achieved the best performance on the validation set are saved as `best.pt`. We load this file to create our `inference_model`. The path to these weights is typically `runs/detect/<name_used_in_training>/weights/best.pt`. In our specific case, this resolves to `runs/detect/yolov8n_coco8_finetuned/weights/best.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to validation images for inference.\n",
    "# We'll use some images from the validation set of our downloaded COCO8 dataset.\n",
    "validation_image_dir = '/content/datasets/coco8/images/val/' # Absolute path to COCO8 validation images\n",
    "\n",
    "# Initialize a list to store paths of sample images for inference\n",
    "sample_image_files = []\n",
    "\n",
    "# Check if the validation image directory exists\n",
    "if os.path.exists(validation_image_dir):\n",
    "    try:\n",
    "        # Get a list of image filenames (ending with .png, .jpg, or .jpeg)\n",
    "        image_filenames = [f for f in os.listdir(validation_image_dir) \n",
    "                           if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        # Select the first 3 images for demonstration, if available\n",
    "        sample_image_files = [os.path.join(validation_image_dir, f) for f in image_filenames[:3]] \n",
    "        if not sample_image_files:\n",
    "            print(f\"No images found in {validation_image_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing images from {validation_image_dir}: {e}\")\n",
    "else:\n",
    "    print(f\"Error: Validation image directory not found at '{validation_image_dir}'. Please check the path.\")\n",
    "\n",
    "# Proceed with inference only if the model loaded successfully and sample images were found\n",
    "if inference_model and sample_image_files:\n",
    "    print(f\"Performing inference on {len(sample_image_files)} sample images...\")\n",
    "    # Run inference using the predict method.\n",
    "    # 'source' can be a single image path, a list of image paths, or a directory.\n",
    "    # 'save=True' will save the images with predictions (bounding boxes, labels, scores).\n",
    "    # 'conf=0.5' sets the confidence threshold; only detections with score > 0.5 will be considered.\n",
    "    results = inference_model.predict(source=sample_image_files, save=True, conf=0.5) \n",
    "    print(f\"Inference complete. Results are saved by default in a 'runs/detect/predict*' directory.\")\n",
    "    # The 'results' object itself is a list of Results objects, containing detailed detection data.\n",
    "elif not inference_model:\n",
    "    print(\"Skipping inference as the fine-tuned model was not loaded successfully.\")\n",
    "else:\n",
    "    print(\"Skipping inference as no sample images were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.predict()` (or in our case, `inference_model.predict()`) function is used to perform object detection on new images or videos.\n",
    "\n",
    "**Key `predict()` Parameters:**\n",
    "- `source`: This can be a path to a single image, a list of image paths, a path to a directory containing images, a path to a video file, or even a live camera feed ID.\n",
    "- `save`: If set to `True` (default is `False`), the images (or video frames) with the drawn bounding boxes, class labels, and confidence scores will be saved. They are typically stored in a new directory like `runs/detect/predict`, `runs/detect/predict2`, and so on.\n",
    "- `conf`: This is the confidence threshold. Only detections with a confidence score greater than this value will be considered valid. A common starting point is 0.25, but it can be adjusted (e.g., to 0.5 as used here) depending on the desired trade-off between recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images with detections.\n",
    "# We will load and show the images that YOLOv8 saved during the predict(save=True) step.\n",
    "\n",
    "# Proceed only if the model was loaded and inference was attempted on some images.\n",
    "if inference_model and sample_image_files:\n",
    "    # Base directory where YOLOv8 saves prediction runs\n",
    "    saved_results_base_dir = 'runs/detect/'\n",
    "    predict_dirs = [] # To store paths of all 'predict*' directories\n",
    "\n",
    "    # Check if the base results directory exists\n",
    "    if os.path.exists(saved_results_base_dir):\n",
    "        # List all directories starting with 'predict' (e.g., predict, predict2, etc.)\n",
    "        predict_dirs = sorted([\n",
    "            os.path.join(saved_results_base_dir, d) \n",
    "            for d in os.listdir(saved_results_base_dir) \n",
    "            if d.startswith('predict') and os.path.isdir(os.path.join(saved_results_base_dir, d))\n",
    "        ])\n",
    "    \n",
    "    # If any 'predict*' directories were found\n",
    "    if predict_dirs:\n",
    "        latest_predict_dir = predict_dirs[-1] # Get the path to the most recent prediction directory\n",
    "        print(f\"Showing results from the latest prediction directory: {latest_predict_dir}\")\n",
    "        \n",
    "        # List all image files within the latest prediction directory\n",
    "        result_image_paths = [os.path.join(latest_predict_dir, f) \n",
    "                              for f in os.listdir(latest_predict_dir) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        if result_image_paths:\n",
    "            # Determine how many images to display (up to 3 or the number available)\n",
    "            num_images_to_display = min(len(result_image_paths), 3)\n",
    "            \n",
    "            plt.figure(figsize=(20, 15)) # Set a larger figure size for better visibility\n",
    "            for i, img_path in enumerate(result_image_paths[:num_images_to_display]):\n",
    "                # Read the image using OpenCV\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    # Convert image from BGR (OpenCV default) to RGB (Matplotlib default)\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "                    \n",
    "                    # Create a subplot for each image\n",
    "                    plt.subplot(1, num_images_to_display, i + 1)\n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.axis('off') # Hide axes for cleaner display\n",
    "                    plt.title(os.path.basename(img_path)) # Show filename as title\n",
    "                else:\n",
    "                    print(f\"Warning: Could not read image {img_path}\")\n",
    "            plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
    "            plt.show() # Display the figure with subplots\n",
    "        else:\n",
    "            print(f\"No result images found in {latest_predict_dir}. Ensure 'model.predict(save=True)' ran correctly.\")\n",
    "    else:\n",
    "        print(f\"No prediction directory (e.g., 'runs/detect/predict*') found. Make sure 'model.predict(save=True)' was executed.\")\n",
    "else:\n",
    "    print(\"Skipping display of results as either the model was not loaded or no images were processed for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above is responsible for visualizing the detection results. It works as follows:\n",
    "1.  It locates the most recent prediction directory created by `inference_model.predict(save=True)` (e.g., `runs/detect/predict`, `runs/detect/predict2`, etc.).\n",
    "2.  It lists the image files (JPG, PNG) within that directory.\n",
    "3.  It then uses `matplotlib.pyplot` to display up to the first 3 of these images in the notebook output.\n",
    "4.  OpenCV (`cv2`) is used to read the images, and they are converted from BGR (OpenCV's default color order) to RGB (Matplotlib's expected color order) for correct color display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook provided a step-by-step guide to performing object detection using YOLOv8. We have successfully:\n",
    "\n",
    "1.  **Set up the environment** by installing `ultralytics`, `torch`, and `opencv-python-headless`.\n",
    "2.  **Prepared the data** by downloading the **COCO8 dataset**, inspecting its structure, and creating a `custom_data.yaml` file with appropriate paths and class names for training.\n",
    "3.  **Fine-tuned a pre-trained YOLOv8n model** on our custom COCO8 dataset for 25 epochs, leveraging transfer learning.\n",
    "4.  **Performed inference** using the fine-tuned model on sample validation images from COCO8 and visualized the detection results, including bounding boxes and class labels.\n",
    "\n",
    "### Potential Next Steps:\n",
    "\n",
    "- **Experiment with different YOLOv8 models:** Try larger models like `yolov8s.pt` or `yolov8m.pt` for potentially higher accuracy, or explore specialized models if available.\n",
    "- **Train for more epochs:** Increase the number of training epochs (e.g., 100, 200, or more) and monitor validation metrics to find the optimal training duration and avoid overfitting (early stopping with `patience` helps here).\n",
    "- **Use a larger or different dataset:** Apply this workflow to other datasets, or expand the current dataset with more images and annotations.\n",
    "- **Hyperparameter tuning:** Experiment with different learning rates, batch sizes, image sizes (`imgsz`), and data augmentation techniques to optimize model performance.\n",
    "- **Evaluate model performance:** Use YOLOv8's evaluation mode (`model.val()`) to get detailed metrics like mAP (mean Average Precision) on a test set to quantitatively assess the model's accuracy.\n",
    "- **Export and deploy the model:** Export the trained model to formats like ONNX or TensorRT for deployment in various applications or edge devices.\n",
    "- **Explore other YOLOv8 features:** Investigate other capabilities of YOLOv8, such as segmentation, classification, or pose estimation, if your task requires them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
