{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 Object Detection Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates a complete workflow for object detection using YOLOv8, a state-of-the-art, real-time object detection model. We will cover the essential steps from setting up the environment to performing inference with a fine-tuned model.\n",
    "\n",
    "**Dataset:** We will use the \"Objects Medical Supplies\" dataset from Roboflow, which is a subset of PASCAL VOC images, focused on medical-related items. This dataset is conveniently available in YOLO format.\n",
    "\n",
    "**Steps Covered:**\n",
    "1.  **Setup:** Installing necessary libraries (`ultralytics` for YOLOv8, `PyTorch`, `OpenCV`).\n",
    "2.  **Data Preparation:** Downloading the dataset, understanding its structure, and preparing the `data.yaml` configuration file required for YOLOv8 training.\n",
    "3.  **Model Training (Fine-tuning):** Loading a pre-trained YOLOv8 model and fine-tuning it on our custom dataset.\n",
    "4.  **Inference and Visualization:** Using the fine-tuned model to detect objects in new images and visualizing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ultralytics for YOLOv8\n",
    "# This package contains the YOLOv8 model implementation and utilities.\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install PyTorch and torchvision\n",
    "# YOLOv8 is built on PyTorch. Torchvision provides useful image transformation tools.\n",
    "!pip install torch torchvision\n",
    "\n",
    "# Install OpenCV for image processing\n",
    "# OpenCV is used for image loading, manipulation, and displaying results.\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above installs the essential packages: `ultralytics` for YOLOv8, `torch` and `torchvision` as its deep learning framework, and `opencv-python-headless` for image processing tasks. Using `opencv-python-headless` is often preferred in server or container environments where GUI features are not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PASCAL VOC (Visual Object Classes) dataset is a widely recognized benchmark for object detection. For this notebook, we use a smaller, more specialized subset called \"Objects Medical Supplies\" from Roboflow. This allows for quicker demonstration of the training and inference pipeline. The dataset is provided in YOLO format, which simplifies the preparation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset (Medical Supplies - a PASCAL VOC subset from Roboflow)\n",
    "# The dataset is already conveniently in YOLO format.\n",
    "# The '-L' flag for curl follows redirects.\n",
    "!curl -L \"https://app.roboflow.com/ds/HzLEpSGavy?key=9B2P1CLD5P\" > roboflow.zip\n",
    "\n",
    "# Unzip the dataset\n",
    "# The '-q' flag makes unzip operate quietly.\n",
    "# The '-d VOC_subset' extracts files into the 'VOC_subset' directory.\n",
    "!unzip -q roboflow.zip -d VOC_subset\n",
    "\n",
    "# Remove the zip file after extraction to save space\n",
    "!rm roboflow.zip \n",
    "\n",
    "# List the contents of the dataset directory to understand its structure\n",
    "!ls VOC_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been downloaded and unzipped into the `VOC_subset` directory. \n",
    "Typically, a YOLO formatted dataset contains:\n",
    "- `train/`: Directory with training images and corresponding label files.\n",
    "- `valid/`: Directory with validation images and corresponding label files.\n",
    "- `test/` (optional): Directory with test images and corresponding label files.\n",
    "- Each image file (e.g., `image.jpg`) has a corresponding text file (`image.txt`) in a `labels` subdirectory, where each line in the text file defines a bounding box (`class_id x_center y_center width height`).\n",
    "- `data.yaml`: A configuration file that specifies paths to train/val/test sets, number of classes, and class names. The Roboflow download includes this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of `data.yaml`\n",
    "\n",
    "The `data.yaml` file is crucial for YOLOv8 as it provides the model with essential information about the dataset:\n",
    "\n",
    "- `train`: Path to the directory containing training images (or a text file listing training images).\n",
    "- `val`: Path to the directory containing validation images (or a text file listing validation images).\n",
    "- `test` (optional): Path to the directory containing test images.\n",
    "- `nc`: The total number of distinct object classes in the dataset.\n",
    "- `names`: A list of strings, where each string is the name of an object class. The order of names corresponds to their class IDs (0 to `nc-1`).\n",
    "\n",
    "While Roboflow provides a `data.yaml`, we will inspect it and then create our own `custom_data.yaml`. This ensures that the paths are correctly specified relative to our notebook environment (using absolute paths is a robust way to handle this, especially in Colab or similar platforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Path to the data.yaml file provided by Roboflow within the unzipped dataset\n",
    "roboflow_yaml_path = 'VOC_subset/data.yaml'\n",
    "\n",
    "# Initialize variables for class count and names\n",
    "nc = 0\n",
    "names = []\n",
    "\n",
    "if os.path.exists(roboflow_yaml_path):\n",
    "    # Open and load the YAML file from Roboflow\n",
    "    with open(roboflow_yaml_path, 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "        print(\"--- Content of Roboflow's data.yaml ---\")\n",
    "        print(yaml.dump(data_config))\n",
    "        \n",
    "        # Extract the number of classes (nc) and class names\n",
    "        nc = data_config['nc']\n",
    "        names = data_config['names']\n",
    "else:\n",
    "    # Error message if Roboflow's YAML is not found (should not happen with this dataset link)\n",
    "    print(f\"Error: {roboflow_yaml_path} not found! Cannot automatically determine class names and number.\")\n",
    "    print(\"Please ensure the dataset downloaded and extracted correctly.\")\n",
    "\n",
    "# Define paths for our custom data.yaml. \n",
    "# These paths should point to the image directories within 'VOC_subset'.\n",
    "# Using os.path.abspath ensures that YOLO can find the data regardless of the script's CWD.\n",
    "base_dir = os.getcwd() # Current working directory of the notebook\n",
    "train_images_path = os.path.join(base_dir, 'VOC_subset/train/images')\n",
    "val_images_path = os.path.join(base_dir, 'VOC_subset/valid/images')\n",
    "test_images_path = os.path.join(base_dir, 'VOC_subset/test/images') \n",
    "\n",
    "# Create the dictionary for our custom data.yaml\n",
    "custom_data_yaml_content = {\n",
    "    'train': train_images_path,\n",
    "    'val': val_images_path,\n",
    "    'nc': nc,\n",
    "    'names': names\n",
    "}\n",
    "\n",
    "# Add the test set path if the directory exists and is not empty\n",
    "if os.path.exists(test_images_path) and os.listdir(test_images_path):\n",
    "    custom_data_yaml_content['test'] = test_images_path\n",
    "else:\n",
    "    # Print a message if the test set is not found or empty\n",
    "    print(f\"Test images directory '{test_images_path}' not found or is empty, omitting from custom_data.yaml.\")\n",
    "\n",
    "# Define the name for our custom YAML file\n",
    "output_yaml_path = 'custom_data.yaml'\n",
    "\n",
    "# Write the custom_data.yaml file\n",
    "with open(output_yaml_path, 'w') as f:\n",
    "    yaml.dump(custom_data_yaml_content, f, sort_keys=False, default_flow_style=None)\n",
    "\n",
    "print(f\"\\n--- Content of created {output_yaml_path} ---\")\n",
    "with open(output_yaml_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above performs two key actions:\n",
    "1.  It inspects the `data.yaml` file that came with the Roboflow dataset to extract the number of classes (`nc`) and the list of class `names`.\n",
    "2.  It then creates a new `custom_data.yaml` file in the notebook's root directory. This new YAML file uses absolute paths for the `train`, `val`, and (if available) `test` image directories. Using absolute paths (via `os.path.abspath` or `os.path.join(os.getcwd(), ...)`) is a good practice because it makes the dataset configuration robust to changes in the current working directory, which can sometimes occur in notebook environments or complex scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training (Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os # os is already imported, but good to have it here if this cell is run independently\n",
    "\n",
    "# Load a pre-trained YOLOv8 model.\n",
    "# 'yolov8n.pt' is the nano version, which is small and fast, suitable for quick demonstrations.\n",
    "# Other versions like 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt' offer higher accuracy at the cost of size and speed.\n",
    "# Using a '.pt' file loads pre-trained weights. Using a '.yaml' file (e.g., 'yolov8n.yaml') would build the model from scratch based on its architecture definition.\n",
    "model = YOLO('yolov8n.pt') \n",
    "\n",
    "# Display model information (optional, but useful for verification)\n",
    "# This shows the model architecture, number of layers, parameters, and GFLOPs.\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading `yolov8n.pt`, which is a YOLOv8 nano model pre-trained on the COCO dataset. By using a pre-trained model, we leverage **transfer learning**.\n",
    "\n",
    "**Benefits of Transfer Learning:**\n",
    "- **Faster Training:** The model has already learned general visual features (edges, textures, basic shapes) from a large dataset. This allows it to learn the specifics of our new, smaller dataset much faster.\n",
    "- **Less Data Required:** Fine-tuning often achieves good results with less data than training a model from scratch, as the foundational feature extraction capabilities are already present.\n",
    "- **Potentially Better Performance:** Pre-trained models provide a strong starting point, which can lead to better overall performance, especially when the custom dataset is small or similar in nature to the pre-training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the custom_data.yaml file created in the previous step.\n",
    "# This file is located in the root directory of our notebook environment.\n",
    "data_yaml_path = 'custom_data.yaml' \n",
    "\n",
    "# Train (fine-tune) the model on our custom dataset.\n",
    "results = model.train(\n",
    "    data=data_yaml_path,  # Path to our custom_data.yaml file.\n",
    "    epochs=25,            # Number of training epochs. Keep low for a quick demo (e.g., 25-50). For production, this would be higher (e.g., 100-300).\n",
    "    imgsz=640,            # Input image size. Images will be resized to 640x640 pixels.\n",
    "    batch=8,              # Batch size. Adjust based on available GPU memory. Common values are 8, 16, 32.\n",
    "    name='yolov8n_medical_supplies_finetuned', # Name for the training run. Results will be saved in 'runs/detect/yolov8n_medical_supplies_finetuned'.\n",
    "    patience=5            # Early stopping patience. Training will stop if no improvement in validation metrics is observed for 5 consecutive epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.train()` function initiates the fine-tuning process. \n",
    "\n",
    "**Key Training Parameters Explained:**\n",
    "- `data`: Specifies the path to our `custom_data.yaml` file, which defines the dataset structure and class information.\n",
    "- `epochs`: The number of complete passes through the entire training dataset. For this demonstration, we use 25 epochs. In real-world scenarios, this value might be significantly higher (e.g., 100, 200, or more), depending on the dataset's size and complexity, and monitored by observing validation metrics.\n",
    "- `imgsz`: The input image size (height and width) for the model. All training images are resized to this dimension (e.g., 640x640 pixels).\n",
    "- `batch`: The number of images processed together in one forward/backward pass during training. This should be set based on available GPU memory; larger batch sizes can sometimes lead to more stable gradients but require more memory.\n",
    "- `name`: A custom name for this specific training run. YOLOv8 will create a directory with this name (e.g., `runs/detect/yolov8n_medical_supplies_finetuned`) to store all training artifacts, including model weights, logs, and visualizations.\n",
    "- `patience`: Enables early stopping. If the validation metrics do not show improvement for a specified number of epochs (here, 5), the training process will halt. This helps prevent overfitting and can save computational resources if the model has converged.\n",
    "\n",
    "All training results, including the best model weights (`best.pt`), final model weights (`last.pt`), performance metrics (like mAP), confusion matrices, and example validation images with predictions, will be saved in a directory structured as `runs/detect/<name_argument_value>` (e.g., `runs/detect/yolov8n_medical_supplies_finetuned`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for inference and display\n",
    "from ultralytics import YOLO # YOLO class for model loading and prediction\n",
    "import cv2                  # OpenCV for image reading and color conversion\n",
    "import matplotlib.pyplot as plt # Matplotlib for displaying images in the notebook\n",
    "import os                   # For path manipulation and directory listing\n",
    "\n",
    "# Path to the best trained model weights.\n",
    "# This path is determined by the 'name' argument used during 'model.train()'.\n",
    "# The 'best.pt' file contains the model weights that achieved the best validation performance.\n",
    "fine_tuned_model_path = 'runs/detect/yolov8n_medical_supplies_finetuned/weights/best.pt'\n",
    "\n",
    "# Initialize model variable\n",
    "inference_model = None \n",
    "\n",
    "# Check if the trained model file exists\n",
    "if not os.path.exists(fine_tuned_model_path):\n",
    "    print(f\"Error: Trained model weights not found at {fine_tuned_model_path}\")\n",
    "    print(\"Please ensure that:\")\n",
    "    print(\"1. The training process in the previous section completed successfully.\")\n",
    "    print(\"2. The 'name' argument in model.train() ('yolov8n_medical_supplies_finetuned') matches the path used here.\")\n",
    "    print(\"3. The 'best.pt' file was generated in the specified directory.\")\n",
    "    # If the model isn't found, inference_model remains None, and subsequent cells should handle this.\n",
    "else:\n",
    "    # Load the fine-tuned model for inference\n",
    "    inference_model = YOLO(fine_tuned_model_path)\n",
    "    print(f\"Successfully loaded fine-tuned model from {fine_tuned_model_path}\")\n",
    "    # Optionally, display model information to confirm it's the correct one\n",
    "    # inference_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model weights that achieved the best performance on the validation set are saved as `best.pt`. We load this file to create our `inference_model`. The path to these weights is typically `runs/detect/<name_used_in_training>/weights/best.pt`. In our specific case, this resolves to `runs/detect/yolov8n_medical_supplies_finetuned/weights/best.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to validation images for inference.\n",
    "# We'll use some images from the validation set of our downloaded 'VOC_subset' dataset.\n",
    "validation_image_dir = 'VOC_subset/valid/images/' \n",
    "\n",
    "# Initialize a list to store paths of sample images for inference\n",
    "sample_image_files = []\n",
    "\n",
    "# Check if the validation image directory exists\n",
    "if os.path.exists(validation_image_dir):\n",
    "    try:\n",
    "        # Get a list of image filenames (ending with .png, .jpg, or .jpeg)\n",
    "        image_filenames = [f for f in os.listdir(validation_image_dir) \n",
    "                           if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        # Select the first 3 images for demonstration, if available\n",
    "        sample_image_files = [os.path.join(validation_image_dir, f) for f in image_filenames[:3]] \n",
    "        if not sample_image_files:\n",
    "            print(f\"No images found in {validation_image_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing images from {validation_image_dir}: {e}\")\n",
    "else:\n",
    "    print(f\"Error: Validation image directory not found at '{validation_image_dir}'. Please check the path.\")\n",
    "\n",
    "# Proceed with inference only if the model loaded successfully and sample images were found\n",
    "if inference_model and sample_image_files:\n",
    "    print(f\"Performing inference on {len(sample_image_files)} sample images...\")\n",
    "    # Run inference using the predict method.\n",
    "    # 'source' can be a single image path, a list of image paths, or a directory.\n",
    "    # 'save=True' will save the images with predictions (bounding boxes, labels, scores).\n",
    "    # 'conf=0.5' sets the confidence threshold; only detections with score > 0.5 will be considered.\n",
    "    results = inference_model.predict(source=sample_image_files, save=True, conf=0.5) \n",
    "    print(f\"Inference complete. Results are saved by default in a 'runs/detect/predict*' directory.\")\n",
    "    # The 'results' object itself is a list of Results objects, containing detailed detection data.\n",
    "elif not inference_model:\n",
    "    print(\"Skipping inference as the fine-tuned model was not loaded successfully.\")\n",
    "else:\n",
    "    print(\"Skipping inference as no sample images were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.predict()` (or in our case, `inference_model.predict()`) function is used to perform object detection on new images or videos.\n",
    "\n",
    "**Key `predict()` Parameters:**\n",
    "- `source`: This can be a path to a single image, a list of image paths, a path to a directory containing images, a path to a video file, or even a live camera feed ID.\n",
    "- `save`: If set to `True` (default is `False`), the images (or video frames) with the drawn bounding boxes, class labels, and confidence scores will be saved. They are typically stored in a new directory like `runs/detect/predict`, `runs/detect/predict2`, and so on.\n",
    "- `conf`: This is the confidence threshold. Only detections with a confidence score greater than this value will be considered valid. A common starting point is 0.25, but it can be adjusted (e.g., to 0.5 as used here) depending on the desired trade-off between recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images with detections.\n",
    "# We will load and show the images that YOLOv8 saved during the predict(save=True) step.\n",
    "\n",
    "# Proceed only if the model was loaded and inference was attempted on some images.\n",
    "if inference_model and sample_image_files:\n",
    "    # Base directory where YOLOv8 saves prediction runs\n",
    "    saved_results_base_dir = 'runs/detect/'\n",
    "    predict_dirs = [] # To store paths of all 'predict*' directories\n",
    "\n",
    "    # Check if the base results directory exists\n",
    "    if os.path.exists(saved_results_base_dir):\n",
    "        # List all directories starting with 'predict' (e.g., predict, predict2, etc.)\n",
    "        predict_dirs = sorted([\n",
    "            os.path.join(saved_results_base_dir, d) \n",
    "            for d in os.listdir(saved_results_base_dir) \n",
    "            if d.startswith('predict') and os.path.isdir(os.path.join(saved_results_base_dir, d))\n",
    "        ])\n",
    "    \n",
    "    # If any 'predict*' directories were found\n",
    "    if predict_dirs:\n",
    "        latest_predict_dir = predict_dirs[-1] # Get the path to the most recent prediction directory\n",
    "        print(f\"Showing results from the latest prediction directory: {latest_predict_dir}\")\n",
    "        \n",
    "        # List all image files within the latest prediction directory\n",
    "        result_image_paths = [os.path.join(latest_predict_dir, f) \n",
    "                              for f in os.listdir(latest_predict_dir) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        if result_image_paths:\n",
    "            # Determine how many images to display (up to 3 or the number available)\n",
    "            num_images_to_display = min(len(result_image_paths), 3)\n",
    "            \n",
    "            plt.figure(figsize=(20, 15)) # Set a larger figure size for better visibility\n",
    "            for i, img_path in enumerate(result_image_paths[:num_images_to_display]):\n",
    "                # Read the image using OpenCV\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    # Convert image from BGR (OpenCV default) to RGB (Matplotlib default)\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "                    \n",
    "                    # Create a subplot for each image\n",
    "                    plt.subplot(1, num_images_to_display, i + 1)\n",
    "                    plt.imshow(img_rgb)\n",
    "                    plt.axis('off') # Hide axes for cleaner display\n",
    "                    plt.title(os.path.basename(img_path)) # Show filename as title\n",
    "                else:\n",
    "                    print(f\"Warning: Could not read image {img_path}\")\n",
    "            plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
    "            plt.show() # Display the figure with subplots\n",
    "        else:\n",
    "            print(f\"No result images found in {latest_predict_dir}. Ensure 'model.predict(save=True)' ran correctly.\")\n",
    "    else:\n",
    "        print(f\"No prediction directory (e.g., 'runs/detect/predict*') found. Make sure 'model.predict(save=True)' was executed.\")\n",
    "else:\n",
    "    print(\"Skipping display of results as either the model was not loaded or no images were processed for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above is responsible for visualizing the detection results. It works as follows:\n",
    "1.  It locates the most recent prediction directory created by `inference_model.predict(save=True)` (e.g., `runs/detect/predict`, `runs/detect/predict2`, etc.).\n",
    "2.  It lists the image files (JPG, PNG) within that directory.\n",
    "3.  It then uses `matplotlib.pyplot` to display up to the first 3 of these images in the notebook output.\n",
    "4.  OpenCV (`cv2`) is used to read the images, and they are converted from BGR (OpenCV's default color order) to RGB (Matplotlib's expected color order) for correct color display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook provided a step-by-step guide to performing object detection using YOLOv8. We have successfully:\n",
    "\n",
    "1.  **Set up the environment** by installing `ultralytics`, `torch`, and `opencv-python-headless`.\n",
    "2.  **Prepared the data** by downloading the \"Objects Medical Supplies\" dataset from Roboflow, inspecting its structure, and creating a `custom_data.yaml` file with appropriate paths for training.\n",
    "3.  **Fine-tuned a pre-trained YOLOv8n model** on our custom dataset for 25 epochs, leveraging transfer learning.\n",
    "4.  **Performed inference** using the fine-tuned model on sample validation images and visualized the detection results, including bounding boxes and class labels.\n",
    "\n",
    "### Potential Next Steps:\n",
    "\n",
    "- **Experiment with different YOLOv8 models:** Try larger models like `yolov8s.pt` or `yolov8m.pt` for potentially higher accuracy, or explore specialized models if available.\n",
    "- **Train for more epochs:** Increase the number of training epochs (e.g., 100, 200, or more) and monitor validation metrics to find the optimal training duration and avoid overfitting (early stopping with `patience` helps here).\n",
    "- **Use a larger or different dataset:** Apply this workflow to other datasets, or expand the current dataset with more images and annotations.\n",
    "- **Hyperparameter tuning:** Experiment with different learning rates, batch sizes, image sizes (`imgsz`), and data augmentation techniques to optimize model performance.\n",
    "- **Evaluate model performance:** Use YOLOv8's evaluation mode (`model.val()`) to get detailed metrics like mAP (mean Average Precision) on a test set to quantitatively assess the model's accuracy.\n",
    "- **Export and deploy the model:** Export the trained model to formats like ONNX or TensorRT for deployment in various applications or edge devices.\n",
    "- **Explore other YOLOv8 features:** Investigate other capabilities of YOLOv8, such as segmentation, classification, or pose estimation, if your task requires them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
